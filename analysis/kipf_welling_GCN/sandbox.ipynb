{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try get the main working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph took 1.0898327191670736 mins\n",
      "Loading features & labels took 4.987367932001749 mins\n",
      "Saving data took 0.011354374885559081 mins\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Put arXiv data into right form for the GCN \"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import arxiv_public_data.tests.intra_citation as ia \n",
    "import time, json, gzip\n",
    "import pickle as pkl\n",
    "from arxiv_public_data.oai_metadata import load_metadata\n",
    "\n",
    "\n",
    "                                #Auxiliary\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def sync_G_with_metadata(G,m):\n",
    "    \"\"\" Citation graph G is missing some articles\n",
    "        (the ones for which we have no full-text data)\n",
    "        So, I need to add these missing ones as isolated\n",
    "        nodes into G.\n",
    "        \n",
    "        G = nx.Graph, citation graph\n",
    "        m = meta data  (get from load_metadata function)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Check if G subset of m. \n",
    "    #It should be; if not, remove the bad noes\n",
    "    G_nodes = list(G.nodes())\n",
    "    ids = [x['id'] for x in m]\n",
    "    bad_nodes = set(G_nodes) - set(ids)\n",
    "    for node in bad_nodes:\n",
    "        G.remove_node(node)\n",
    "        \n",
    "    #Then find missing nodes & add them\n",
    "    missing = set(ids) - set(G_nodes)\n",
    "    for node in missing:\n",
    "        if not G.has_node(node):\n",
    "            G.add_node(node)\n",
    "    \n",
    "    #Check if they're sync'd up\n",
    "    print('(#Nodes in G, #articles in meta-data) = ' + str((G.number_of_nodes(),len(m))))\n",
    "    if G.number_of_nodes() == len(m):\n",
    "        print('So the syncing worked')\n",
    "    else:\n",
    "        print('So the syncing did not work')\n",
    "    return G\n",
    "\n",
    "\n",
    "def clean_labels(labels):\n",
    "    \"\"\" Some labels have multiple listings\n",
    "        so I take the first one\n",
    "        \n",
    "        Input: list of strings\n",
    "    \"\"\"\n",
    "\n",
    "    for i,label in enumerate(labels):\n",
    "\n",
    "        #If multiple listings, take first\n",
    "        label = label[0].split()[0]\n",
    "\n",
    "        #Merge sub-classes\n",
    "        label = label[:label.find('.')]\n",
    "\n",
    "        labels[i] = label\n",
    "    return labels\n",
    "\n",
    "\n",
    "def labels2categorical(labels):\n",
    "    \"\"\" labels are strings -- have form\n",
    "        'hep-th' -- So need to covert to\n",
    "        categoricals\n",
    "    \"\"\"\n",
    "        \n",
    "    #Create mapping\n",
    "    classes = set(labels)\n",
    "    class_labels = {}\n",
    "    for i,x in enumerate(classes):\n",
    "        class_labels[x] = i\n",
    "    class_labels\n",
    "    \n",
    "    #change\n",
    "    labels_categorical = []\n",
    "    for label in labels:\n",
    "        vec = np.zeros(len(classes))\n",
    "        temp = class_labels[label]\n",
    "        vec[temp] = 1\n",
    "        labels_categorical.append(vec)\n",
    "    return np.array(labels_categorical)\n",
    "\n",
    "\n",
    "def load_titles(G, m, nodes_string, dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/title-embedding-usel-2019-03-19.pkl'\n",
    "    out = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                out.extend(pkl.load(f))\n",
    "            except EOFError as e:\n",
    "                break\n",
    "    title_vecs = np.array(out)\n",
    "    \n",
    "    #Then select the subset corresponding to the sub-graph we're examining\n",
    "    if len(nodes_string) == G.number_of_nodes():\n",
    "        return title_vecs\n",
    "    else:\n",
    "        indicies = []\n",
    "        all_nodes = [x['id'] for x in m]\n",
    "        for i,node in enumerate(nodes_string):\n",
    "            index = all_nodes.index(node)\n",
    "            indicies.append(index)\n",
    "        return title_vecs[indicies]\n",
    "\n",
    "\n",
    "def load_abstracts(G, m, nodes_string, dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/abstract-embedding-usel-2019-03-19.pkl'\n",
    "    out = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                out.extend(pkl.load(f))\n",
    "            except EOFError as e:\n",
    "                break\n",
    "    abstract_vecs = np.array(out)\n",
    "    \n",
    "    #Then select the subset corresponding to the sub-graph we're examining\n",
    "    if len(nodes_string) == G.number_of_nodes():\n",
    "        return abstract_vecs\n",
    "    else:\n",
    "        indicies = []\n",
    "        all_nodes = [x['id'] for x in m]\n",
    "        for i,node in enumerate(nodes_string):\n",
    "            index = all_nodes.index(node)\n",
    "            indicies.append(index)\n",
    "        return abstract_vecs[indicies]\n",
    "\n",
    "\n",
    "def load_fulltext(G, m, nodes_string, dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/fulltext-embedding-usel-2-headers-2019-04-05.pkl'\n",
    "    out = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                out.extend(pkl.load(f))\n",
    "            except EOFError as e:\n",
    "                break\n",
    "    fulltext_vecs = np.array(out)\n",
    "    \n",
    "    #Then select the subset corresponding to the sub-graph we're examining\n",
    "    if len(nodes_string) == G.number_of_nodes():\n",
    "        return fulltext_vecs\n",
    "    else:\n",
    "        indicies = []\n",
    "        all_nodes = [x['id'] for x in m]\n",
    "        for i,node in enumerate(nodes_string):\n",
    "            index = all_nodes.index(node)\n",
    "            indicies.append(index)\n",
    "        return fulltext_vecs[indicies]\n",
    "\n",
    "\n",
    "def load_labels(nodes_string, m):\n",
    "    labels = [x['categories'] for x in m if x['id'] in nodes_string]\n",
    "    labels_cl = clean_labels(labels)\n",
    "    labels_cat = labels2categorical(labels_cl)\n",
    "    return labels_cat\n",
    "\n",
    "\n",
    "def save_data(nodes_int, dirname,vector_type, vector_train, vector_test, vector, G_sub, m):\n",
    "    \"\"\" Saves data in format required by Kipfs and Welling\n",
    "    \n",
    "        nodes_int = list, list of nodes labeled by integers\n",
    "        dirname = string, where to save the data\n",
    "        vector_type = string, = 'title', 'abstract', 'full-text'\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #Save vectors\n",
    "    dirname = 'data'\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.x'\n",
    "    pkl.dump(vector_train, open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.tx'\n",
    "    pkl.dump(vector_test, open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.allx'\n",
    "    pkl.dump(vector[:cutoff2], open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.y'\n",
    "    pkl.dump(np.array(labels_train), open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.ty'\n",
    "    pkl.dump(np.array(labels_test), open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.ally'\n",
    "    pkl.dump(np.array(labels_cat[:cutoff2]), open(fname,'wb'))\n",
    "\n",
    "    test_nodes = nodes_int[cutoff2:]\n",
    "    with open(dirname + '/ind.arXiv-' + vector_type + '.test.index','wt') as f:\n",
    "        for node in test_nodes:\n",
    "            f.write(str(node))\n",
    "            f.write('\\n')\n",
    "            \n",
    "            \n",
    "    #Save graph in format required by Kipf-Welling -- nodes labeled as ints\n",
    "    #Also, need to save in same order as metadata\n",
    "    #So need to relabel the nodes\n",
    "    \n",
    "    #Find mapping: article_id -->int\n",
    "    #mapping = {}\n",
    "    #for i,item in enumerate(m):\n",
    "    #    article_id = item['id']\n",
    "    #    mapping[article_id] = i\n",
    "\n",
    "    #Apply mapping\n",
    "    #nx.relabel_nodes(G_sub, mapping, copy = False)\n",
    "    \n",
    "    #Save\n",
    "    G_sub = nx.convert_node_labels_to_integers(G_sub)\n",
    "    graph_dict = {}     \n",
    "    for node in G_sub.nodes():\n",
    "        graph_dict[node] = list(G_sub.neighbors(node))\n",
    "    pkl.dump(graph_dict, open(dirname + '/ind.arXiv-' + vector_type + '.graph', 'wb')) \n",
    "    return\n",
    "    \n",
    "\n",
    "                                #Main\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    N = 2*10**3  #size of subgraph to be used (using a subraph during testing phase)\n",
    "    #N = 0 #This means take all of the data\n",
    "    \n",
    "    #Load graph & metadata\n",
    "    t1 = time.time()\n",
    "    \n",
    "    #Meta data -- need this later\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data'\n",
    "    m = load_metadata(dirname + '/oai-arxiv-metadata-2019-03-01.json.gz')\n",
    "    \n",
    "    graph_saved = True    #for debugging, I've already made the full graph\n",
    "    if graph_saved == True:\n",
    "        dirname = 'data'\n",
    "        G = nx.read_gpickle(dirname + '/full_graph.gpickle')\n",
    "    else:\n",
    "        dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data/output'\n",
    "        fname = dirname + '/internal-citations.json.gz'\n",
    "        q = json.load(gzip.open(fname, 'rt', encoding='utf-8'))\n",
    "        G = ia.makegraph(q)\n",
    "        G = sync_G_with_metadata(G,m)   #G is missing some articles (see func for more details)\n",
    "\n",
    "    #Select subgraph if specified\n",
    "    if N != 0:\n",
    "        comps = nx.weakly_connected_components(G)\n",
    "        biggest = max(comps, key=len)\n",
    "        G_cc = G.subgraph(biggest)\n",
    "        nodes = list(G_cc.nodes())[:N]\n",
    "        G_sub = G_cc.subgraph(nodes)\n",
    "    else:\n",
    "        G_sub = G\n",
    "        \n",
    "    nodes_string = list(G_sub.nodes())            #nodes labeled by ints\n",
    "    nodes_int = range(G_sub.number_of_nodes())    #nodes labeled in strings\n",
    "    t2 = time.time()\n",
    "    print('Loading graph took ' + str((t2-t1)/60.0) + ' mins')\n",
    "\n",
    "    #Load features\n",
    "    t1 = time.time()\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data/output/embeddings'\n",
    "    title_vecs = load_titles(G, m, nodes_string, dirname)\n",
    "    abstract_vecs = load_abstracts(G, m, nodes_string, dirname)\n",
    "    #fulltext_vecs = load_fulltext(G, m, nodes_string, dirname)\n",
    "    \n",
    "    #Load labels\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data'\n",
    "    labels_cat = load_labels(nodes_string, m)\n",
    "    t2 = time.time()\n",
    "    print( 'Loading features & labels took ' + str((t2-t1)/60.0) + ' mins')\n",
    "\n",
    "    t1 = time.time()\n",
    "    #Split into test & train & ulabeled portion\n",
    "    #For now, I'll assume that nothing is unlabeled\n",
    "    #That means cutoff1 and cutoff2 are the same\n",
    "    cutoff1 = int(0.9*title_vecs.shape[0]) \n",
    "    cutoff2 = int(0.9*title_vecs.shape[0])\n",
    "    title_vec_train, title_vec_test = title_vecs[:cutoff1], title_vecs[cutoff2:]\n",
    "    abstract_vec_train, abstract_vec_test = abstract_vecs[:cutoff1], abstract_vecs[cutoff2:]\n",
    "    fulltext_vec_train, fulltext_vec_test = fulltext_vecs[:cutoff1], fulltext_vecs[cutoff2:]\n",
    "    labels_train, labels_test = labels_cat[:cutoff1], labels_cat[cutoff2:]\n",
    "\n",
    "    #Save data\n",
    "    dirname = 'data'\n",
    "    save_data(nodes_int, dirname, 'title', title_vec_train, title_vec_test, title_vecs, G_sub, m)\n",
    "    save_data(nodes_int, dirname, 'abstract', abstract_vec_train, abstract_vec_test, abstract_vecs, G_sub, m)\n",
    "    #save_data(nodes_int, dirname, 'fulltext', fulltext_vec_train, fulltext_vec_test, fulltext_vecs, G_sub, m)\n",
    "    \n",
    "    #Combine title and abstract vecs\n",
    "    title_vecs = np.concatenate((title_vecs, abstract_vecs), axis=1)\n",
    "    title_vec_train, title_vec_test = title_vecs[:cutoff1], title_vecs[cutoff2:]\n",
    "    #save_data(nodes_int, dirname, 'title-abstract', title_vec_train, title_vec_test, title_vecs, G_sub, m)\n",
    "    t2 = time.time()\n",
    "    print( 'Saving data took ' + str((t2-t1)/60.0) + ' mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughwork -- fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data/output/embeddings'\n",
    "fulltext_vecs = load_fulltext(G, m, nodes_string, dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulltext_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vecs = load_titles(G, m, nodes_string, dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 512), (2000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_vecs.shape, fulltext_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colins way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'fill_zeros' from 'arxiv_public_data.embeddings.util' (/home/kokeeffe/research/arxiv-public-datasets/arxiv_public_data/embeddings/util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a962cbe3a00f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0marxiv_public_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_zeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfulltext_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'fill_zeros' from 'arxiv_public_data.embeddings.util' (/home/kokeeffe/research/arxiv-public-datasets/arxiv_public_data/embeddings/util.py)"
     ]
    }
   ],
   "source": [
    "from arxiv_public_data.embeddings.util import load_embeddings, fill_zeros\n",
    "\n",
    "fulltext_vectors = fill_zeros(load_embeddings(filename, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
