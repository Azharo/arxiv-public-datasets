{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try get the main working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-01 00:27:28,362 - arxivdata.config - WARNING: default output directory is /home/kokeeffe/research/arxiv-public-datasets/analysis/kipf_welling_GCN/arxiv-data\n"
     ]
    }
   ],
   "source": [
    "import GCN_utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_data() missing 13 required positional arguments: 'nodes_int', 'dirname', 'vector_type', 'vector_train', 'vector_test', 'vector', 'labels_train', 'labels_test', 'labels_cat', 'G_sub', 'm', 'cutoff1', and 'cutoff2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5d500bb49195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save_data() missing 13 required positional arguments: 'nodes_int', 'dirname', 'vector_type', 'vector_train', 'vector_test', 'vector', 'labels_train', 'labels_test', 'labels_cat', 'G_sub', 'm', 'cutoff1', and 'cutoff2'"
     ]
    }
   ],
   "source": [
    "u.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e88f1e78b4de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/internal-citations.json.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakegraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msync_G_with_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#G is missing some articles (see func for more details)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/arxiv-public-datasets/analysis/intra_citation.py\u001b[0m in \u001b[0;36mmakegraph\u001b[0;34m(data, clean, directed)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_cite_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclean\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mref\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mart\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/networkx/classes/digraph.py\u001b[0m in \u001b[0;36madd_edge\u001b[0;34m(self, u_of_edge, v_of_edge, **attr)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;31m# add the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0mdatadict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0mdatadict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_succ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Put arXiv data into right form for the GCN \"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import intra_citation as ia \n",
    "import time, json, gzip\n",
    "import pickle as pkl\n",
    "from arxiv_public_data.oai_metadata import load_metadata\n",
    "from arxiv_public_data.embeddings.util import load_embeddings, fill_zeros\n",
    "\n",
    "\n",
    "                                #Auxiliary\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def makegraph(data, clean=True, directed=True):\n",
    "    G = nx.DiGraph() if directed else nx.Graph()\n",
    "    G.add_nodes_from(data.keys())\n",
    "    bad_refs = []\n",
    "    for art in data.keys():\n",
    "        for ref in data[art]:\n",
    "            if ref in data:\n",
    "                ref = clean_cite_name(ref) if clean else ref\n",
    "            if not ref == art and ref in data:\n",
    "                G.add_edge(art, ref)\n",
    "            elif clean_cite_name(ref) in data and clean:\n",
    "                G.add_edge(art, clean_cite_name(ref))\n",
    "            else:\n",
    "                bad_refs.append((art, clean_cite_name(ref)))\n",
    "    return G, bad_refs\n",
    "\n",
    "\n",
    "def sync_G_with_metadata(G,m):\n",
    "    \"\"\" Citation graph G is missing some articles\n",
    "        (the ones for which we have no full-text data)\n",
    "        So, I need to add these missing ones as isolated\n",
    "        nodes into G.\n",
    "        \n",
    "        G = nx.Graph, citation graph\n",
    "        m = meta data  (get from load_metadata function)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Check if G subset of m. \n",
    "    #It should be; if not, remove the bad noes\n",
    "    G_nodes = list(G.nodes())\n",
    "    ids = [x['id'] for x in m]\n",
    "    bad_nodes = set(G_nodes) - set(ids)\n",
    "    for node in bad_nodes:\n",
    "        G.remove_node(node)\n",
    "        \n",
    "    #Then find missing nodes & add them\n",
    "    missing = set(ids) - set(G_nodes)\n",
    "    for node in missing:\n",
    "        if not G.has_node(node):\n",
    "            G.add_node(node)\n",
    "    \n",
    "    #Check if they're sync'd up\n",
    "    print('(#Nodes in G, #articles in meta-data) = ' + str((G.number_of_nodes(),len(m))))\n",
    "    if G.number_of_nodes() == len(m):\n",
    "        print('So the syncing worked')\n",
    "    else:\n",
    "        print('So the syncing did not work')\n",
    "    return G\n",
    "\n",
    "\n",
    "def clean_labels(labels):\n",
    "    \"\"\" Some labels have multiple listings\n",
    "        so I take the first one\n",
    "        \n",
    "        Input: list of strings\n",
    "    \"\"\"\n",
    "\n",
    "    for i,label in enumerate(labels):\n",
    "\n",
    "        #If multiple listings, take first\n",
    "        label = label[0].split()[0]\n",
    "\n",
    "        #Merge sub-classes\n",
    "        label = label[:label.find('.')]\n",
    "\n",
    "        labels[i] = label\n",
    "    return labels\n",
    "\n",
    "\n",
    "def labels2categorical(labels):\n",
    "    \"\"\" labels are strings -- have form\n",
    "        'hep-th' -- So need to covert to\n",
    "        categoricals\n",
    "    \"\"\"\n",
    "        \n",
    "    #Create mapping\n",
    "    classes = set(labels)\n",
    "    class_labels = {}\n",
    "    for i,x in enumerate(classes):\n",
    "        class_labels[x] = i\n",
    "    class_labels\n",
    "    \n",
    "    #change\n",
    "    labels_categorical = []\n",
    "    for label in labels:\n",
    "        vec = np.zeros(len(classes))\n",
    "        temp = class_labels[label]\n",
    "        vec[temp] = 1\n",
    "        labels_categorical.append(vec)\n",
    "    return np.array(labels_categorical)\n",
    "\n",
    "\n",
    "def load_titles(G, m, nodes_string, dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/title-embedding-usel-2019-03-19.pkl'\n",
    "    title_vecs =  load_embeddings(filename)['embeddings']\n",
    "    \n",
    "    #Then select the subset corresponding to the sub-graph we're examining\n",
    "    if len(nodes_string) == G.number_of_nodes():\n",
    "        return title_vecs\n",
    "    else:\n",
    "        indicies = []\n",
    "        all_nodes = [x['id'] for x in m]\n",
    "        for i,node in enumerate(nodes_string):\n",
    "            index = all_nodes.index(node)\n",
    "            indicies.append(index)\n",
    "        return title_vecs[indicies]\n",
    "\n",
    "\n",
    "def load_abstracts(G, m, nodes_string, dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/abstract-embedding-usel-2019-03-19.pkl'\n",
    "    abstract_vecs =  load_embeddings(filename)['embeddings']\n",
    "    \n",
    "    #Then select the subset corresponding to the sub-graph we're examining\n",
    "    if len(nodes_string) == G.number_of_nodes():\n",
    "        return abstract_vecs\n",
    "    else:\n",
    "        indicies = []\n",
    "        all_nodes = [x['id'] for x in m]\n",
    "        for i,node in enumerate(nodes_string):\n",
    "            index = all_nodes.index(node)\n",
    "            indicies.append(index)\n",
    "        return abstract_vecs[indicies]\n",
    "\n",
    "\n",
    "def load_fulltext(G, m, nodes_string, dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data/output/embeddings'\n",
    "    filename = dirname + '/fulltext-embedding-usel-2-headers-2019-04-05.pkl'\n",
    "    fulltext_vecs = fill_zeros(load_embeddings(filename, 2))\n",
    "\n",
    "    #Then select the subset corresponding to the sub-graph we're examining\n",
    "    if len(nodes_string) == G.number_of_nodes():\n",
    "        return fulltext_vecs\n",
    "    else:\n",
    "        indicies = []\n",
    "        all_nodes = [x['id'] for x in m]\n",
    "        for i,node in enumerate(nodes_string):\n",
    "            index = all_nodes.index(node)\n",
    "            indicies.append(index)\n",
    "        return fulltext_vecs[indicies]\n",
    "\n",
    "\n",
    "def load_labels(nodes_string, m):\n",
    "    labels = [x['categories'] for x in m if x['id'] in nodes_string]\n",
    "    labels_cl = clean_labels(labels)\n",
    "    labels_cat = labels2categorical(labels_cl)\n",
    "    return labels_cat\n",
    "\n",
    "\n",
    "def save_data(nodes_int, dirname,vector_type, vector_train, vector_test, vector, G_sub, m):\n",
    "    \"\"\" Saves data in format required by Kipfs and Welling\n",
    "    \n",
    "        nodes_int = list, list of nodes labeled by integers\n",
    "        dirname = string, where to save the data\n",
    "        vector_type = string, = 'title', 'abstract', 'full-text'\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #Save vectors\n",
    "    dirname = 'data'\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.x'\n",
    "    pkl.dump(vector_train, open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.tx'\n",
    "    pkl.dump(vector_test, open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.allx'\n",
    "    pkl.dump(vector[:cutoff2], open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.y'\n",
    "    pkl.dump(np.array(labels_train), open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.ty'\n",
    "    pkl.dump(np.array(labels_test), open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.ally'\n",
    "    pkl.dump(np.array(labels_cat[:cutoff2]), open(fname,'wb'))\n",
    "\n",
    "    test_nodes = nodes_int[cutoff2:]\n",
    "    with open(dirname + '/ind.arXiv-' + vector_type + '.test.index','wt') as f:\n",
    "        for node in test_nodes:\n",
    "            f.write(str(node))\n",
    "            f.write('\\n')\n",
    "            \n",
    "            \n",
    "    #Save graph in format required by Kipf-Welling -- nodes labeled as ints\n",
    "    #Also, need to save in same order as metadata\n",
    "    #So need to relabel the nodes\n",
    "    \n",
    "    #Find mapping: article_id -->int\n",
    "    #mapping = {}\n",
    "    #for i,item in enumerate(m):\n",
    "    #    article_id = item['id']\n",
    "    #    mapping[article_id] = i\n",
    "\n",
    "    #Apply mapping\n",
    "    #nx.relabel_nodes(G_sub, mapping, copy = False)\n",
    "    \n",
    "    #Save\n",
    "    G_sub = nx.convert_node_labels_to_integers(G_sub)\n",
    "    graph_dict = {}     \n",
    "    for node in G_sub.nodes():\n",
    "        graph_dict[node] = list(G_sub.neighbors(node))\n",
    "    pkl.dump(graph_dict, open(dirname + '/ind.arXiv-' + vector_type + '.graph', 'wb')) \n",
    "    return\n",
    "    \n",
    "\n",
    "                                #Main\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    N = 10**2  #size of subgraph to be used (using a subraph during testing phase)\n",
    "    #N = 0 #This means take all of the data\n",
    "    \n",
    "    #Load graph & metadata\n",
    "    t1 = time.time()\n",
    "    \n",
    "    #Meta data -- need this later\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data'\n",
    "    m = load_metadata(dirname + '/oai-arxiv-metadata-2019-03-01.json.gz')\n",
    "    \n",
    "    graph_saved = False    #for debugging, I've already made the full graph\n",
    "    if graph_saved == True:\n",
    "        dirname = 'data'\n",
    "        G = nx.read_gpickle(dirname + '/full_graph.gpickle')\n",
    "    else:\n",
    "        dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data/output'\n",
    "        fname = dirname + '/internal-citations.json.gz'\n",
    "        q = json.load(gzip.open(fname, 'rt', encoding='utf-8'))\n",
    "        G = ia.makegraph(q)\n",
    "        G = sync_G_with_metadata(G,m)   #G is missing some articles (see func for more details)\n",
    "\n",
    "    #Select subgraph if specified\n",
    "    if N != 0:\n",
    "        comps = nx.weakly_connected_components(G)\n",
    "        biggest = max(comps, key=len)\n",
    "        G_cc = G.subgraph(biggest)\n",
    "        nodes = list(G_cc.nodes())[:N]\n",
    "        G_sub = G_cc.subgraph(nodes)\n",
    "    else:\n",
    "        G_sub = G\n",
    "        \n",
    "    nodes_string = list(G_sub.nodes())            #nodes labeled by ints\n",
    "    nodes_int = range(G_sub.number_of_nodes())    #nodes labeled in strings\n",
    "    t2 = time.time()\n",
    "    print('Loading graph took ' + str((t2-t1)/60.0) + ' mins')\n",
    "\n",
    "    #Load features\n",
    "    t1 = time.time()\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data/output/embeddings'\n",
    "    title_vecs = load_titles(G, m, nodes_string, dirname)\n",
    "    abstract_vecs = load_abstracts(G, m, nodes_string, dirname)\n",
    "    fulltext_vecs = load_fulltext(G, m, nodes_string, dirname)\n",
    "    \n",
    "    #Load labels\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data'\n",
    "    labels_cat = load_labels(nodes_string, m)\n",
    "    t2 = time.time()\n",
    "    print( 'Loading features & labels took ' + str((t2-t1)/60.0) + ' mins')\n",
    "\n",
    "    t1 = time.time()\n",
    "    #Split into test & train & ulabeled portion\n",
    "    #For now, I'll assume that nothing is unlabeled\n",
    "    #That means cutoff1 and cutoff2 are the same\n",
    "    cutoff1 = int(0.9*title_vecs.shape[0]) \n",
    "    cutoff2 = int(0.9*title_vecs.shape[0])\n",
    "    title_vec_train, title_vec_test = title_vecs[:cutoff1], title_vecs[cutoff2:]\n",
    "    abstract_vec_train, abstract_vec_test = abstract_vecs[:cutoff1], abstract_vecs[cutoff2:]\n",
    "    fulltext_vec_train, fulltext_vec_test = fulltext_vecs[:cutoff1], fulltext_vecs[cutoff2:]\n",
    "    labels_train, labels_test = labels_cat[:cutoff1], labels_cat[cutoff2:]\n",
    "\n",
    "    #Save data\n",
    "    dirname = 'data'\n",
    "    save_data(nodes_int, dirname, 'title', title_vec_train, title_vec_test, title_vecs, G_sub, m)\n",
    "    save_data(nodes_int, dirname, 'abstract', abstract_vec_train, abstract_vec_test, abstract_vecs, G_sub, m)\n",
    "    save_data(nodes_int, dirname, 'fulltext', fulltext_vec_train, fulltext_vec_test, fulltext_vecs, G_sub, m)\n",
    "    \n",
    "    #Combine all\n",
    "    title_vecs = np.concatenate([title_vecs, abstract_vecs, fulltext_vecs], axis=1)\n",
    "    title_vec_train, title_vec_test = title_vecs[:cutoff1], title_vecs[cutoff2:]\n",
    "    save_data(nodes_int, dirname, 'all', title_vec_train, title_vec_test, title_vecs, G_sub, m)\n",
    "    t2 = time.time()\n",
    "    print( 'Saving data took ' + str((t2-t1)/60.0) + ' mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
