{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Full text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-29 14:39:04,542 - arxivdata.config - WARNING: default output directory is /home/kokeeffe/research/arxiv-public-datasets/arxiv_public_data/tests/kipf_welling_GCN/arxiv-data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#Nodes in citation graph G, #articles in meta-data) = (1506500, 1506500)\n",
      "So the syncing worked\n",
      "Loading graph took 1.2567044099171956 mins\n",
      "Loading features & labels took 0.4854541182518005 mins\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot serialize a bytes object larger than 4 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-82d5a37f07bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mtitle_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mtitle_vec_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_vec_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitle_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcutoff1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcutoff2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title-abstract'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_vec_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_vec_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-82d5a37f07bf>\u001b[0m in \u001b[0;36msave_data\u001b[0;34m(dirname, vector_type, vector_train, vector_test, vector, G, m)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mdirname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/ind.arXiv-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvector_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.x'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/ind.arXiv-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvector_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.tx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot serialize a bytes object larger than 4 GiB"
     ]
    }
   ],
   "source": [
    "\"\"\" Put arXiv data into right form for the GCN \"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import arxiv_public_data.tests.intra_citation as ia \n",
    "import time, json, gzip\n",
    "import pickle as pkl\n",
    "from arxiv_public_data.oai_metadata import load_metadata\n",
    "\n",
    "\n",
    "                                #Auxiliary\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def sync_G_with_metadata(G,m):\n",
    "    \"\"\" Citation graph G is missing some articles\n",
    "        (the ones for which we have no full-text data)\n",
    "        So, I need to add these missing ones as isolated\n",
    "        nodes into G.\n",
    "        \n",
    "        G = nx.Graph, citation graph\n",
    "        m = meta data  (get from load_metadata function)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Check if G subset of m. \n",
    "    #It should be; if not, remove the bad noes\n",
    "    G_nodes = list(G.nodes())\n",
    "    ids = [x['id'] for x in m]\n",
    "    bad_nodes = set(G_nodes) - set(ids)\n",
    "    for node in bad_nodes:\n",
    "        G.remove_node(node)\n",
    "        \n",
    "    #Then find missing nodes & add them\n",
    "    missing = set(ids) - set(G_nodes)\n",
    "    for node in missing:\n",
    "        if not G.has_node(node):\n",
    "            G.add_node(node)\n",
    "    \n",
    "    #Check if they're sync'd up\n",
    "    print('(#Nodes in citation graph G, #articles in meta-data) = ' + str((G.number_of_nodes(),len(m))))\n",
    "    if G.number_of_nodes() == len(m):\n",
    "        print('So the syncing worked')\n",
    "    else:\n",
    "        print('So the syncing did not work')\n",
    "    return G\n",
    "\n",
    "\n",
    "def clean_labels(labels):\n",
    "    \"\"\" Some labels have multiple listings\n",
    "        so I take the first one\n",
    "        \n",
    "        Input: list of strings\n",
    "    \"\"\"\n",
    "\n",
    "    for i,label in enumerate(labels):\n",
    "\n",
    "        #If multiple listings, take first\n",
    "        label = label[0].split()[0]\n",
    "\n",
    "        #Merge sub-classes\n",
    "        label = label[:label.find('.')]\n",
    "\n",
    "        labels[i] = label\n",
    "    return labels\n",
    "\n",
    "\n",
    "def labels2categorical(labels):\n",
    "    \"\"\" labels are strings -- have form\n",
    "        'hep-th' -- So need to covert to\n",
    "        categoricals\n",
    "    \"\"\"\n",
    "        \n",
    "    #Create mapping\n",
    "    classes = set(labels)\n",
    "    class_labels = {}\n",
    "    for i,x in enumerate(classes):\n",
    "        class_labels[x] = i\n",
    "    class_labels\n",
    "    \n",
    "    #change\n",
    "    labels_categorical = []\n",
    "    for label in labels:\n",
    "        vec = np.zeros(len(classes))\n",
    "        temp = class_labels[label]\n",
    "        vec[temp] = 1\n",
    "        labels_categorical.append(vec)\n",
    "    return np.array(labels_categorical)\n",
    "\n",
    "\n",
    "def load_titles(dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/title-embedding-usel-2019-03-19.pkl'\n",
    "    out = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                out.extend(pkl.load(f))\n",
    "            except EOFError as e:\n",
    "                break\n",
    "    title_vecs = np.array(out)\n",
    "    return title_vecs\n",
    "    \n",
    "\n",
    "\n",
    "def load_abstracts(dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/abstract-embedding-usel-2019-03-19.pkl'\n",
    "    out = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                out.extend(pkl.load(f))\n",
    "            except EOFError as e:\n",
    "                break\n",
    "    abstract_vecs = np.array(out)\n",
    "    return abstract_vecs\n",
    "\n",
    "\n",
    "def load_fulltext(dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/fulltext-embedding-usel-2-headers-2019-04-05.pkl'\n",
    "    out = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                out.extend(pkl.load(f))\n",
    "            except EOFError as e:\n",
    "                break\n",
    "    fulltext_vecs = np.array(out)\n",
    "    return fulltext_vecs\n",
    "\n",
    "\n",
    "def load_labels(m):\n",
    "    labels = [x['categories'] for x in m]\n",
    "    labels_cl = clean_labels(labels)\n",
    "    labels_cat = labels2categorical(labels_cl)\n",
    "    return labels_cat\n",
    "\n",
    "\n",
    "def save_data(dirname,vector_type, vector_train, vector_test, vector, G, m):\n",
    "    \"\"\" Saves data in format required by Kipfs and Welling\n",
    "    \n",
    "        nodes_int = list, list of nodes labeled by integers\n",
    "        dirname = string, where to save the data\n",
    "        vector_type = string, = 'title', 'abstract', 'full-text'\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #Save vectors\n",
    "    dirname = 'data'\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.x'\n",
    "    pkl.dump(vector_train, open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.tx'\n",
    "    pkl.dump(vector_test, open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.allx'\n",
    "    pkl.dump(vector[:cutoff2], open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.y'\n",
    "    pkl.dump(np.array(labels_train), open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.ty'\n",
    "    pkl.dump(np.array(labels_test), open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.ally'\n",
    "    pkl.dump(np.array(labels_cat[:cutoff2]), open(fname,'wb'))\n",
    "\n",
    "    nodes_as_ints = range(G.number_of_nodes())  #kipf welling label nodes as ints\n",
    "    test_nodes = nodes_as_ints[cutoff2:]\n",
    "    with open(dirname + '/ind.arXiv-' + vector_type + '.test.index','wt') as f:\n",
    "        for node in test_nodes:\n",
    "            f.write(str(node))\n",
    "            f.write('\\n')\n",
    "            \n",
    "            \n",
    "    #Save graph in format required by Kipf-Welling\n",
    "    #Also, need to save in same order as metadata\n",
    "    #Note, I used protocal 4 with pickle, since there's \n",
    "    # a 4GB limit on what you can pickle without it.\n",
    "    graph_dict = {}     \n",
    "    for item in m:\n",
    "        node = item['id']\n",
    "        graph_dict[node] = list(G.neighbors(node))\n",
    "    pkl.dump(graph_dict, open(dirname + '/ind.arXiv-' + vector_type + '.graph', 'wb'), protocol=4) \n",
    "    return\n",
    "    \n",
    "\n",
    "\n",
    "                                #Main\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    #Graph\n",
    "    t1 = time.time()\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data/output'\n",
    "    fname = dirname + '/internal-citations.json.gz'\n",
    "    q = json.load(gzip.open(fname, 'rt', encoding='utf-8'))\n",
    "    G = ia.makegraph(q)\n",
    "    \n",
    "    #Meta data\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data'\n",
    "    m = load_metadata( dirname + '/oai-arxiv-metadata-2019-03-01.json.gz')\n",
    "    \n",
    "    #Make sure G is sync'd with the metadata (has same of the nodes)\n",
    "    G = sync_G_with_metadata(G,m)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print('Loading graph took ' + str((t2-t1)/60.0) + ' mins')\n",
    "\n",
    "\n",
    "    #Load features\n",
    "    t1 = time.time()\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data/output/embeddings'\n",
    "    title_vecs = load_titles(dirname)\n",
    "    abstract_vecs = load_abstracts(dirname)\n",
    "    #fulltext_vecs = load_fulltext(dirname)\n",
    "    \n",
    "    #Load labels\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data'\n",
    "    labels_cat = load_labels(m)\n",
    "    t2 = time.time()\n",
    "    print( 'Loading features & labels took ' + str((t2-t1)/60.0) + ' mins')\n",
    "\n",
    "    #Split into test & train & ulabeled portion\n",
    "    #For now, I'll assume that nothing is unlabeled\n",
    "    #That means cutoff1 and cutoff2 are the same\n",
    "    t1 = time.time()\n",
    "    cutoff1 = int(0.9*title_vecs.shape[0]) \n",
    "    cutoff2 = int(0.9*title_vecs.shape[0])\n",
    "    title_vec_train, title_vec_test = title_vecs[:cutoff1], title_vecs[cutoff2:]\n",
    "    abstract_vec_train, abstract_vec_test = abstract_vecs[:cutoff1], abstract_vecs[cutoff2:]\n",
    "    #fulltext_vec_train, fulltext_vec_test = fulltext_vecs[:cutoff1], fulltext_vecs[cutoff2:]\n",
    "    labels_train, labels_test = labels_cat[:cutoff1], labels_cat[cutoff2:]\n",
    "\n",
    "    #Save data\n",
    "    dirname = 'data'\n",
    "    save_data(dirname, 'title', title_vec_train, title_vec_test, title_vecs, G, m)\n",
    "    save_data(dirname, 'abstract', abstract_vec_train, abstract_vec_test, abstract_vecs, G, m)\n",
    "\n",
    "    #Combine title and abstract vecs\n",
    "    title_vecs = np.concatenate((title_vecs, abstract_vecs), axis=1)\n",
    "    title_vec_train, title_vec_test = title_vecs[:cutoff1], title_vecs[cutoff2:]\n",
    "    save_data(dirname, 'title-abstract', title_vec_train, title_vec_test, title_vecs, G, m)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print( 'Saving data took ' + str((t2-t1)/60.0) + ' mins')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
